{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import mpu\n",
    "from urllib.error import HTTPError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = '/home/tobamo/analize/project-tobamo/results/megan6_results_combined.csv'\n",
    "\n",
    "# Load CSV file into DataFrame and clean NaN values\n",
    "cleaned_df = pd.read_csv(file_path, index_col=0).dropna(subset=['nr_sseqid'])\n",
    "\n",
    "# Extract unique 'nr_sseqid' values and split before dot\n",
    "nr_sseqid = [str(sseqid).split('.')[0] for sseqid in cleaned_df['nr_sseqid'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_taxonomies_in_batch(accession_dict, db, batch_size):\n",
    "    if batch_size > 100:\n",
    "        raise ValueError(\"Batch size exceeds the limit of 100.\")\n",
    "    \n",
    "    accession_list = [key for key, val in accession_dict.items() if val == \"\"]\n",
    "    batch_accessions = accession_list[:batch_size]\n",
    "\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=db, id=batch_accessions, retmode='xml')\n",
    "        records = Entrez.read(handle)\n",
    "        for record in records:\n",
    "            accession = record[\"GBSeq_primary-accession\"]\n",
    "            accession_dict[accession] = record[\"GBSeq_taxonomy\"]\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTP Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    return accession_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'accession_dict'\n",
    "Entrez.email = \"neza.pajekarambasic@fri.uni-lj.si\"\n",
    "\n",
    "accession_dict = {\"\":\"\"}\n",
    "\n",
    "# Fetch taxonomies until all values are filled\n",
    "while \"\" in accession_dict.values():\n",
    "    try:\n",
    "        accession_dict = mpu.io.read(f'{output_name}.json')\n",
    "    except FileNotFoundError:\n",
    "        accession_dict = {accession: \"\" for accession in nr_sseqid}\n",
    "    accession_dict = fetch_taxonomies_in_batch(accession_dict, \"protein\", 100) # batch_size <= 100\n",
    "    mpu.io.write(f'{output_name}.json', accession_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
